{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review and tutorial of \n",
    "\n",
    "# Deep Neural Networks as Gaussian Processes \n",
    "Jaehoon Lee, Yasaman Bahri, Roman Novak , Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein\n",
    "  \n",
    "#### Presented by  \n",
    "[Jason Deng](mailto:dengzj@Hotmail.com)  \n",
    "[Alexander Dubitskiy](mailto:ald028@g.harvard.edu)  \n",
    "[Zheng Yang](mailto:zhengyang@g.harvard.edu)  \n",
    "[Sean Tierney](mailto:set936@g.harvard.edu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question of defining meaningful priors for a neural network was first addressed by Radford M. Neal in his 1994 paper [Priors for infinite networks](ftp://www.cs.toronto.edu/dist/radford/pin.pdf).  \n",
    "In the paper he studied a neural network with a real valued input of size I, one hidden layer of size H with sigmoidal transfer function and real valued output of size O. This network can be described with the following equations:\n",
    "$$ f_k(x) = b_k + \\sum_{i=1}^{H} v_{jk} h_j(x) $$ \n",
    "$$ h_j(x) = tanh( a_j + \\sum_{i=1}^{I} u_{ij} x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author suggested Gaussian priors over the network weights $ b_k \\propto \\mathcal{N}(0, \\sigma_b) , v_{jk} \\propto \\mathcal{N}(0, \\sigma_v), a_j \\propto \\mathcal{N}(0, \\sigma_a) , u_{ij} \\propto \\mathcal{N}(0, \\sigma_u) $ and showed that when $H \\to \\infty$ the prior joint distribution converges to a multivariate Gaussian with zero means and covarinace:\n",
    "  \n",
    "$$ E[f_k(x) f_k(x\\prime)]  = \\sigma_{b}^2 + \\sum_{j} \\sigma_{v}^2 E[h_{j}(x)h_{j}(x \\prime)] = \\sigma_{b}^2 + w_{v}^2 C(x, x \\prime) $$\n",
    "and this is a Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work was further extended by Christopher K. I. Williams in his 1996 paper [Computing with infinite networks](https://papers.nips.cc/paper/1197-computing-with-infinite-networks.pdf).  \n",
    "Williams showed how to calculate the corresponding kernels for single-layered neural networks with sigmoidal and Gaussian transfer functions.  \n",
    "  \n",
    "$$ C_{erf}(x, x \\prime) = \\frac{2}{\\pi} \\sin^{-1} \\frac{2 x^{T} \\Sigma x \\prime }{\\sqrt{(1 + 2 x^{T} \\Sigma x ) (1 + 2 x \\prime^{T} \\Sigma x \\prime )}} $$\n",
    "  \n",
    "$$ C_{G}(x, x \\prime) = (\\frac{\\sigma_e}{\\sigma_u})^d \\exp(- \\frac{x^{T}x}{2 \\sigma_{m}^2}) \\exp(- \\frac{(x - x^{T})^{T} (x - x^{T})}{2 \\sigma_{s}^2}) \\exp(- \\frac{x \\prime^{T}x \\prime}{2 \\sigma_{m}^2}) $$  \n",
    "where \n",
    "$$ \\frac{1}{\\sigma_{e}^2} = \\frac{2}{\\sigma_{g}^2} + \\frac{1}{\\sigma_{u}^2} , \\sigma_{s}^2 = 2 \\sigma_{g}^2 + \\frac{\\sigma_{g}^4}{\\sigma_{u}^2} , \\sigma_{m}^2 = 2 \\sigma_{u}^2 + \\sigma_{g}^2 $$  \n",
    "and $ \\sigma_{g} $ is the width of the Gaussian transfer function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Summary of the kernel computation for a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an $L$-hidden-layer fully-connected neural network with hidden layers of width $N_l$ (for layer $l$) and pointwise nonlinearities $\\phi$. Let $x \\in \\Re^{d_{in}}$ denote the input to the network, and let $z^L \\in \\Re^{d_{out}}$ denote it output. The $i$th component of activations in the $l$th layer, post-nonlinearity and post-affine trnsformatin, are denoted $x_i^l$ and $z_i^l$ repectively. Weight and bias parameters for the $l$th layer have components $W_{ij}^l, b_i^l$, which are independent and randomly drawn, and we take them all to have zero mean and variances $\\sigma_w^2/N_l$ and $\\sigma_b^2$, repectively. $GP(\\mu, K)$ denotes a Gaussian process with mean and covariance functions $\\mu(\\cdotp), K(\\cdotp, \\cdotp)$, repectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the multidimentional Central Limit Theorem first.  \n",
    "\n",
    "We assume each infdividual $X_i$ is a random vector in $\\Re^k$, with mean vector $\\mu  = E(X_i)$ and these random vectors are i.i.d.,  \n",
    "$$\\frac{1}{n}\\Sigma^n_{i = 1}X_i = \n",
    "\\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "    \\Sigma^n_{i = 1}X_{i(1)}\\\\\n",
    "    \\vdots \\\\\n",
    "    \\Sigma^n_{i = 1}X_{i(k)}\n",
    "\\end{bmatrix}\n",
    " = \\bar{X_n}$$\n",
    "And therefore, \n",
    "$$\\frac{1}{\\sqrt{n}}\\Sigma^n_{i = 1}[X_i - E(X_i)] = \\frac{1}{\\sqrt{n}}\\Sigma^n_{i = 1}(X_i - \\mu) = \\sqrt{n}(\\bar{X_n} - \\mu).$$  \n",
    "\n",
    "The multidimentional Central Limit Theorem states that\n",
    "$$\\sqrt{n}(\\bar{X_n} - \\mu) \\sim N_k(0, \\Sigma)$$  \n",
    "\n",
    "In a multi-layer neural network, the weights and bias parameters are taken to be i.i.d., and the post-activations $x_j^l, x_{j'}^l$, are independent for $j \\neq j'$. From the multidimentional Center Limit Theorem, as $N_l \\rightarrow \\infty$, any finite collection of $\\{z_i^l(x^{\\alpha=1}), \\ldots, z_i^l(x^{\\alpha=k})\\}$ will have a joint multivariate Guassian distribution and $z_i^l \\sim GP(0, K^l).$  \n",
    "\n",
    "The covariance is \n",
    "$$K^l(x, x') \\equiv E[z^l_i(x)z^l_i(x')] = \\sigma^2_b + \\sigma^2_wE_{z_i^{l-1}\\sim GP(0, K^{l-1})}[\\phi(z_i^{l-1}(x))\\phi(z_i^{l-1}(x'))].$$  \n",
    "\n",
    "Since the expectation in above equation is over the GP governing $z_i^{l-1}$, which is equivalent to integrating against the joint distribution of only $z_i^{l-1}(x)$ and $z_i^{l-1}(x')$. This joint distirbution has zero mean and covariance matrix \n",
    "$$K = \n",
    "\\begin{bmatrix}\n",
    "    K^{l-1}(x, x) & K^{l-1}(x, x')\\\\\n",
    "     K^{l-1}(x, x') &  K^{l-1}(x', x') \\\\\n",
    "\\end{bmatrix}$$  \n",
    "\n",
    "Thus, we can introduce the shorthand\n",
    "$$K^l(x, x') = \\sigma^2_b + \\sigma^2_wF_\\phi(K^{l-1}(x, x'), K^{l-1}(x, x), K^{l-1}(x', x'))$$\n",
    "to emphasize the recursive relationship between $K^l$ and $K^{l-1}$ via a deterministic function F whose form depends only on the nonlinearity $\\phi$. This gives an iterative series of computations which can be performed to obtain $K^L$ for the GP describing the network's final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base case $K^0$, suppose $W^0_{ij} \\sim N(0, \\sigma^2_x/d_{in}), b_j^0 \\sim N(0, \\sigma_b^2)$; we can utilize the recursion relating $K^1$ and $K^0$, where\n",
    "$$K^0(x, x') = E[z_j^0(x)z_j^0(x')] = \\sigma^2_b + \\sigma^2_w(\\frac{x\\cdot x'}{d_{in}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
