{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review and tutorial of \n",
    "\n",
    "# Deep Neural Networks as Gaussian Processes \n",
    "Jaehoon Lee, Yasaman Bahri, Roman Novak , Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein\n",
    "  \n",
    "#### Presented by  \n",
    "[Jason Deng](mailto:dengzj@Hotmail.com)  \n",
    "[Alexander Dubitskiy](mailto:ald028@g.harvard.edu)  \n",
    "[Zheng Yang](mailto:zhengyang@g.harvard.edu)  \n",
    "[Sean Tierney](mailto:set936@g.harvard.edu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question of defining meaningful priors for a neural network was first addressed by Radford M. Neal in his 1994 paper [Priors for infinite networks](ftp://www.cs.toronto.edu/dist/radford/pin.pdf).  \n",
    "In the paper he studied a neural network with a real valued input of size I, one hidden layer of size H with sigmoidal transfer function and real valued output of size O. This network can be described with the following equations:\n",
    "$$ f_k(x) = b_k + \\sum_{i=1}^{H} v_{jk} h_j(x) $$ \n",
    "$$ h_j(x) = tanh( a_j + \\sum_{i=1}^{I} u_{ij} x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author suggested Gaussian priors over the network weights $ b_k \\propto \\mathcal{N}(0, \\sigma_b) , v_{jk} \\propto \\mathcal{N}(0, \\sigma_v), a_j \\propto \\mathcal{N}(0, \\sigma_a) , u_{ij} \\propto \\mathcal{N}(0, \\sigma_u) $ and showed that when $H \\to \\infty$ the prior joint distribution converges to a multivariate Gaussian with zero means and covarinace:\n",
    "  \n",
    "$$ E[f_k(x) f_k(x\\prime)]  = \\sigma_{b}^2 + \\sum_{j} \\sigma_{v}^2 E[h_{j}(x)h_{j}(x \\prime)] = \\sigma_{b}^2 + w_{v}^2 C(x, x \\prime) $$\n",
    "and this is a Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work was further extended by Christopher K. I. Williams in his 1996 paper [Computing with infinite networks](https://papers.nips.cc/paper/1197-computing-with-infinite-networks.pdf).  \n",
    "Williams showed how to calculate the corresponding kernels for single-layered neural networks with sigmoidal and Gaussian transfer functions.  \n",
    "  \n",
    "$$ C_{erf}(x, x \\prime) = \\frac{2}{\\pi} \\sin^{-1} \\frac{2 x^{T} \\Sigma x \\prime }{\\sqrt{(1 + 2 x^{T} \\Sigma x ) (1 + 2 x \\prime^{T} \\Sigma x \\prime )}} $$\n",
    "  \n",
    "$$ C_{G}(x, x \\prime) = (\\frac{\\sigma_e}{\\sigma_u})^d \\exp(- \\frac{x^{T}x}{2 \\sigma_{m}^2}) \\exp(- \\frac{(x - x^{T})^{T} (x - x^{T})}{2 \\sigma_{s}^2}) \\exp(- \\frac{x \\prime^{T}x \\prime}{2 \\sigma_{m}^2}) $$  \n",
    "where \n",
    "$$ \\frac{1}{\\sigma_{e}^2} = \\frac{2}{\\sigma_{g}^2} + \\frac{1}{\\sigma_{u}^2} , \\sigma_{s}^2 = 2 \\sigma_{g}^2 + \\frac{\\sigma_{g}^4}{\\sigma_{u}^2} , \\sigma_{m}^2 = 2 \\sigma_{u}^2 + \\sigma_{g}^2 $$  \n",
    "and $ \\sigma_{g} $ is the width of the Gaussian transfer function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Summary of the kernel computation for a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an $L$-hidden-layer fully-connected neural network with hidden layers of width $N_l$ (for layer $l$) and pointwise nonlinearities $\\phi$. Let $x \\in \\Re^{d_{in}}$ denote the input to the network, and let $z^L \\in \\Re^{d_{out}}$ denote it output. The $i$th component of activations in the $l$th layer, post-nonlinearity and post-affine trnsformatin, are denoted $x_i^l$ and $z_i^l$ repectively. Weight and bias parameters for the $l$th layer have components $W_{ij}^l, b_i^l$, which are independent and randomly drawn, and we take them all to have zero mean and variances $\\sigma_w^2/N_l$ and $\\sigma_b^2$, repectively. $GP(\\mu, K)$ denotes a Gaussian process with mean and covariance functions $\\mu(\\cdotp), K(\\cdotp, \\cdotp)$, repectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the multidimentional Central Limit Theorem first.  \n",
    "\n",
    "We assume each infdividual $X_i$ is a random vector in $\\Re^k$, with mean vector $\\mu  = E(X_i)$ and these random vectors are i.i.d.,  \n",
    "$$\\frac{1}{n}\\Sigma^n_{i = 1}X_i = \n",
    "\\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "    \\Sigma^n_{i = 1}X_{i(1)}\\\\\n",
    "    \\vdots \\\\\n",
    "    \\Sigma^n_{i = 1}X_{i(k)}\n",
    "\\end{bmatrix}\n",
    " = \\bar{X_n}$$\n",
    "And therefore, \n",
    "$$\\frac{1}{\\sqrt{n}}\\Sigma^n_{i = 1}[X_i - E(X_i)] = \\frac{1}{\\sqrt{n}}\\Sigma^n_{i = 1}(X_i - \\mu) = \\sqrt{n}(\\bar{X_n} - \\mu).$$  \n",
    "\n",
    "The multidimentional Central Limit Theorem states that\n",
    "$$\\sqrt{n}(\\bar{X_n} - \\mu) \\sim N_k(0, \\Sigma)$$  \n",
    "\n",
    "In a multi-layer neural network, the weights and bias parameters are taken to be i.i.d., and the post-activations $x_j^l, x_{j'}^l$, are independent for $j \\neq j'$. From the multidimentional Center Limit Theorem, as $N_l \\rightarrow \\infty$, any finite collection of $\\{z_i^l(x^{\\alpha=1}), \\ldots, z_i^l(x^{\\alpha=k})\\}$ will have a joint multivariate Guassian distribution and $z_i^l \\sim GP(0, K^l).$  \n",
    "\n",
    "The covariance is \n",
    "$$K^l(x, x') \\equiv E[z^l_i(x)z^l_i(x')] = \\sigma^2_b + \\sigma^2_wE_{z_i^{l-1}\\sim GP(0, K^{l-1})}[\\phi(z_i^{l-1}(x))\\phi(z_i^{l-1}(x'))].$$  \n",
    "\n",
    "Since the expectation in above equation is over the GP governing $z_i^{l-1}$, which is equivalent to integrating against the joint distribution of only $z_i^{l-1}(x)$ and $z_i^{l-1}(x')$. This joint distirbution has zero mean and covariance matrix \n",
    "$$K = \n",
    "\\begin{bmatrix}\n",
    "    K^{l-1}(x, x) & K^{l-1}(x, x')\\\\\n",
    "     K^{l-1}(x', x) &  K^{l-1}(x', x') \\\\\n",
    "\\end{bmatrix}$$  \n",
    "\n",
    "Thus, we can introduce the shorthand\n",
    "$$K^l(x, x') = \\sigma^2_b + \\sigma^2_wF_\\phi(K^{l-1}(x, x'), K^{l-1}(x, x), K^{l-1}(x', x'))$$\n",
    "to emphasize the recursive relationship between $K^l$ and $K^{l-1}$ via a deterministic function F whose form depends only on the nonlinearity $\\phi$. This gives an iterative series of computations which can be performed to obtain $K^L$ for the GP describing the network's final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base case $K^0$, suppose $W^0_{ij} \\sim N(0, \\sigma^2_x/d_{in}), b_j^0 \\sim N(0, \\sigma_b^2)$; we can utilize the recursion relating $K^1$ and $K^0$, where\n",
    "$$K^0(x, x') = E[z_j^0(x)z_j^0(x')] = \\sigma^2_b + \\sigma^2_w(\\frac{x\\cdot x'}{d_{in}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the NNGP implementation\n",
    "\n",
    "In a nutshell, this notebook implemented a Gaussian process regression with a NNGP kernel to return predicted Y_test given X_test. The followings are to explain how the implementation allows the computation of the GP regression and the kernel from a reverse-engineering perspective.\n",
    "\n",
    "First of all, the Gaussian Process regression model was based on GPflow, and it is to estimate a GP $p(y^* | x, y, x^*) \\sim N(\\bar{\\mu},\\bar{K})$ according to the following equations (ref: equaiton (8 and 9), Lee etal, V3 March, 2018):\n",
    "\n",
    "$$\n",
    "\\bar{\\mu} = K^L_{x^*, D} (K^L_{D, D} + \\sigma^2 \\mathbb{I}_n)^{-1} t \n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\bar{K}=K^L_{x^*,x^*} - K^L_{x^*,D}(K^L_{D, D} + \\sigma^2 \\mathbb{I}_n)^{-1} K^{L,T}_{x^*, D} \n",
    "$$\n",
    "\n",
    "where $\\bar{\\mu}$ is the mean of predicted Y, $\\bar{K}$ represents the variance of the prediction. The model computes the Cholesky decompositionthe of $K^L$ as part of the algorithm for finding $\\bar{\\mu}$ and $\\bar{K}$, using the functions defined under the class GaussianProcessRegression. This implementation was explained somewhere else (Rasmussen and Williams, Gaussian processes for machine learning, 2006). \n",
    "\n",
    "In order to get the $K^L$ for the model, a NNGP kernel was implemented with The key function k_full(). k_full() first computes $K^l$ that is the covariance of post-activation at given pre-activation variance and correlation. It then returns a fully stacked $K^L$ over all the layers in the neural network. The implementation was based on the following euqation (ref: equaiton (4), Lee etal, V3 March, 2018):\n",
    "\n",
    "$$\n",
    "K^l(x,x') = \\sigma^2_b + \\sigma^2_w \\mathbb{E}_{z^{l-1}_i \\sim GP(0, K^{l-1})}[\\phi(z^{l-1}_i (x))\\phi(z^{l-1}_i (x')]\n",
    "$$\n",
    "\n",
    "where $\\sigma^2_b$ and $\\sigma^2_w$ are initial values of the variance, which are initialized with the kernel. The expectation term $\\mathbb{E}_{z^{l-1}_i \\sim GP(0, K^{l-1})}[\\phi(z^{l-1}_i (x))\\phi(z^{l-1}_i (x')]$, which is a Gaussian integral, was computed via numerical integration implmentation. Briefly, a numerical integration is achieved by the linear interpolation method for all pairs of training-training and training-test points for layer $l$. First, all the inputs are processed to have identical norms. This preprocessing guarantees the identical marginal variance for each datapoint. Next, a matrix F is populated, containing a lookup table for the function $F_\\phi$ , in the following euqation  (ref: equaiton (5), Lee etal, V3 March, 2018):\n",
    "\n",
    "$$\n",
    "K^l(x,x') = \\sigma^2_b + \\sigma^2_w F_\\phi (K^{l-1}(x,x'), K^{l-1}(x,x),K^{l-1}(x',x'))\n",
    "$$\n",
    "\n",
    "The the function $F_\\phi$  was approximately by bilinear interpolation into the matrix F. This is repeated recursively over all the layers. The bilinear interpolation has constant cost. \n",
    "Noted, these paramters are initialzed with the kernel: n_gauss, n_var and n_corr, determining the sampling densities for the numerical integration.  \n",
    "\n",
    "\n",
    "To describe the implementation of k_full() in peudocodes.\n",
    "\n",
    "k_full (): \n",
    "    \n",
    "    Normalize input to unit variance or to fixed point variance\n",
    "    \n",
    "    For each hidden layer l:\n",
    "        q_ab = interp.interp_lin_2d(...) # numerical integration for the expectation of the prevoius layer, E(z^{l-1}) (equation 4)\n",
    "        q_ab = self.weight_var * q_ab + self.bias_var #This computes the covariance of the current layer, K^l\n",
    "        \n",
    "    q_ab_all = parallelly_stack(q_ab)  #K^L over all the layers\n",
    "    \n",
    "    return q_ab_all\n",
    "\n",
    "\n",
    "\n",
    "In addition, the following tensorboard graph captures the workflow of k_full() .\n",
    "<img src=\"kfull.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('nngp')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block defines functions for linear interpretation, used for calculating the function F as described in the implementation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Interpolate in NNGP grid.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def interp_lin(x, y, xp, log_spacing=False):\n",
    "    \"\"\"Linearly interpolate.\n",
    "\n",
    "    x is evenly spaced grid coordinates, with values y,\n",
    "    xp are the locations to which to interpolate.\n",
    "    x and xp must be 1d tensors.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('interp_lin'):\n",
    "        if log_spacing:\n",
    "            x = tf.log(x)\n",
    "            xp = tf.log(xp)\n",
    "\n",
    "        spacing = x[1] - x[0]\n",
    "        grid = (xp - x[0]) / spacing\n",
    "        ind1 = tf.cast(grid, tf.int32)\n",
    "        ind2 = ind1 + 1\n",
    "        max_ind = x.shape[0].value\n",
    "        # set top and bottom indices identical if extending past end of range\n",
    "        ind2 = tf.minimum(max_ind - 1, ind2)\n",
    "\n",
    "        weight1 = tf.abs(xp - tf.gather(x, ind1)) / spacing\n",
    "        weight2 = tf.abs(xp - tf.gather(x, ind2)) / spacing\n",
    "        if log_spacing:\n",
    "            weight1 = tf.exp(weight1)\n",
    "            weight2 = tf.exp(weight2)\n",
    "\n",
    "        weight1 = 1. - tf.reshape(weight1, [-1] + [1] * (len(y.shape) - 1))\n",
    "        weight2 = 1. - tf.reshape(weight2, [-1] + [1] * (len(y.shape) - 1))\n",
    "\n",
    "        weight_sum = weight1 + weight2\n",
    "        weight1 /= weight_sum\n",
    "        weight2 /= weight_sum\n",
    "\n",
    "        y1 = tf.gather(y, ind1)\n",
    "        y2 = tf.gather(y, ind2)\n",
    "        yp = y1 * weight1 + y2 * weight2\n",
    "        return yp\n",
    "\n",
    "\n",
    "def _get_interp_idxs_weights_2d(x, xp, y, yp, x_log_spacing=False):\n",
    "    with tf.name_scope('get_interp_idxs_weights_2d'):\n",
    "        if x_log_spacing:\n",
    "            x = tf.log(x)\n",
    "            xp = tf.log(xp)\n",
    "\n",
    "        with tf.control_dependencies([yp]):\n",
    "            xp = tf.tile(xp, yp.shape)\n",
    "        xyp = tf.expand_dims(tf.parallel_stack([xp, yp]), 1)\n",
    "        xy0 = tf.reshape(tf.parallel_stack([x[0], y[0]]), [2, 1, 1])\n",
    "        xy1 = tf.reshape(tf.parallel_stack([x[1], y[1]]), [2, 1, 1])\n",
    "\n",
    "        spacing = xy1 - xy0\n",
    "        ind_grid = (xyp - xy0) / spacing\n",
    "        ind = tf.cast(ind_grid, tf.int32) + [[[0], [1]]]\n",
    "\n",
    "        max_ind = [[[x.shape[0].value - 1]], [[y.shape[0].value - 1]]]\n",
    "        ind = tf.minimum(ind, max_ind)\n",
    "        ind_float = tf.cast(ind, tf.float64)\n",
    "\n",
    "        xy_grid = ind_float * spacing + xy0\n",
    "\n",
    "        weight = tf.abs(xyp - xy_grid) / spacing\n",
    "        if x_log_spacing:\n",
    "            weight = tf.parallel_stack([tf.exp(weight[0]), weight[1]])\n",
    "        weight = 1. - weight\n",
    "\n",
    "        weight_sum = tf.reduce_sum(weight, axis=1, keep_dims=True)\n",
    "        weight /= weight_sum\n",
    "\n",
    "        return ind, weight\n",
    "\n",
    "\n",
    "def interp_lin_2d(x, y, z, xp, yp, x_log_spacing=False):\n",
    "    with tf.name_scope('interp_lin_2d'):\n",
    "        ind, weight = _get_interp_idxs_weights_2d(x, xp, y, yp, x_log_spacing)\n",
    "        zp_accum = 0.\n",
    "\n",
    "        for ind_x, weight_x in [(ind[0, 0], weight[0, 0]),\n",
    "                                (ind[0, 1], weight[0, 1])]:\n",
    "            for ind_y, weight_y in [(ind[1, 0], weight[1, 0]),\n",
    "                                    (ind[1, 1], weight[1, 1])]:\n",
    "                zp = tf.gather_nd(z, tf.stack([ind_x, ind_y], axis=1))\n",
    "                zp_accum += zp * weight_x * weight_y\n",
    "\n",
    "        return zp_accum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class defined below defines the GP kernel. It's initialized with a number of hyperparameters, including the variance of bias and weight values, number of layers, and nonlinearity. Included are functions for calculating $K^L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Neural Network Gaussian Process (nngp) kernel computation.\n",
    "\n",
    "Implementaion based on\n",
    "\"Deep Neural Networks as Gaussian Processes\" by\n",
    "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz,\n",
    "Jeffrey Pennington, Jascha Sohl-Dickstein\n",
    "arXiv:1711.00165 (https://arxiv.org/abs/1711.00165).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import interp\n",
    "\n",
    "\n",
    "class NNGPKernel(object):\n",
    "    \"\"\"The iterative covariance Kernel for Neural Network Gaussian Process.\n",
    "\n",
    "  Args:\n",
    "    depth: int, number of hidden layers in corresponding NN.\n",
    "    nonlin_fn: tf ops corresponding to point-wise non-linearity in corresponding\n",
    "      NN. e.g.) tf.nn.relu, tf.nn.sigmoid, lambda x: x * tf.nn.sigmoid(x), ...\n",
    "    weight_var: initial value for the weight_variances parameter.\n",
    "    bias_var: initial value for the bias_variance parameter.\n",
    "    n_gauss: Number of gaussian integration grid. Choose odd integer, so that\n",
    "      there is a gridpoint at 0.\n",
    "    n_var: Number of variance grid points.\n",
    "    n_corr: Number of correlation grid points.\n",
    "    use_fixed_point_norm: bool, normalize input to variance fixed point.\n",
    "      Defaults to False, normalizing input to unit norm over input dimension.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 depth=1,\n",
    "                 nonlin_fn=tf.tanh,\n",
    "                 weight_var=1.,\n",
    "                 bias_var=1.,\n",
    "                 n_gauss=101,\n",
    "                 n_var=151,\n",
    "                 n_corr=131,\n",
    "                 max_var=100,\n",
    "                 max_gauss=100,\n",
    "                 use_fixed_point_norm=False,\n",
    "                 fraction_of_int32=32,\n",
    "                 use_precomputed_grid=False,\n",
    "                 grid_path=None,\n",
    "                 sess=None):\n",
    "        self.depth = depth\n",
    "        self.weight_var = weight_var\n",
    "        self.bias_var = bias_var\n",
    "        self.use_fixed_point_norm = use_fixed_point_norm\n",
    "        self.sess = sess\n",
    "        self.fraction_of_int32 = fraction_of_int32\n",
    "        self.use_precomputed_grid = use_precomputed_grid\n",
    "\n",
    "        if use_precomputed_grid and (grid_path is None):\n",
    "            raise ValueError(\"grid_path must be specified to use precomputed grid.\")\n",
    "        self.grid_path = grid_path\n",
    "\n",
    "        self.nonlin_fn = nonlin_fn\n",
    "        (self.var_aa_grid, self.corr_ab_grid, self.qaa_grid,\n",
    "         self.qab_grid) = self.get_grid(n_gauss, n_var, n_corr, max_var, max_gauss)\n",
    "\n",
    "        if self.use_fixed_point_norm:\n",
    "            self.var_fixed_point_np, self.var_fixed_point = self.get_var_fixed_point()\n",
    "\n",
    "    def get_grid(self, n_gauss, n_var, n_corr, max_var, max_gauss):\n",
    "        \"\"\"Get covariance grid by loading or computing a new one.\n",
    "    \"\"\"\n",
    "        # File configuration for precomputed grid\n",
    "        if self.use_precomputed_grid:\n",
    "            grid_path = self.grid_path\n",
    "            # TODO(jaehlee) np.save have broadcasting error when n_var==n_corr.\n",
    "            if n_var == n_corr:\n",
    "                n_var += 1\n",
    "            grid_file_name = \"grid_{0:s}_ng{1:d}_ns{2:d}_nc{3:d}\".format(\n",
    "                self.nonlin_fn.__name__, n_gauss, n_var, n_corr)\n",
    "            grid_file_name += \"_mv{0:d}_mg{1:d}\".format(max_var, max_gauss)\n",
    "\n",
    "        # Load grid file if it exists already\n",
    "        if (self.use_precomputed_grid and\n",
    "                tf.gfile.Exists(os.path.join(grid_path, grid_file_name))):\n",
    "            with tf.gfile.Open(os.path.join(grid_path, grid_file_name), \"rb\") as f:\n",
    "                grid_data_np = np.load(f)\n",
    "                tf.logging.info(\"Loaded interpolation grid from %s\" %\n",
    "                                os.path.join(grid_path, grid_file_name))\n",
    "                grid_data = (tf.convert_to_tensor(grid_data_np[0], dtype=tf.float64),\n",
    "                             tf.convert_to_tensor(grid_data_np[1], dtype=tf.float64),\n",
    "                             tf.convert_to_tensor(grid_data_np[2], dtype=tf.float64),\n",
    "                             tf.convert_to_tensor(grid_data_np[3], dtype=tf.float64))\n",
    "\n",
    "        else:\n",
    "            tf.logging.info(\"Generating interpolation grid...\")\n",
    "            grid_data = _compute_qmap_grid(self.nonlin_fn, n_gauss, n_var, n_corr,\n",
    "                                           max_var=max_var, max_gauss=max_gauss)\n",
    "            if self.use_precomputed_grid:\n",
    "                with tf.Session() as sess:\n",
    "                    grid_data_np = sess.run(grid_data)\n",
    "                tf.gfile.MakeDirs(grid_path)\n",
    "                with tf.gfile.Open(os.path.join(grid_path, grid_file_name), \"wb\") as f:\n",
    "                    np.save(f, grid_data_np)\n",
    "\n",
    "                with tf.gfile.Open(os.path.join(grid_path, grid_file_name), \"rb\") as f:\n",
    "                    grid_data_np = np.load(f)\n",
    "                    tf.logging.info(\"Loaded interpolation grid from %s\" %\n",
    "                                    os.path.join(grid_path, grid_file_name))\n",
    "                    grid_data = (tf.convert_to_tensor(grid_data_np[0], dtype=tf.float64),\n",
    "                                 tf.convert_to_tensor(grid_data_np[1], dtype=tf.float64),\n",
    "                                 tf.convert_to_tensor(grid_data_np[2], dtype=tf.float64),\n",
    "                                 tf.convert_to_tensor(grid_data_np[3], dtype=tf.float64))\n",
    "\n",
    "        return grid_data\n",
    "\n",
    "    def get_var_fixed_point(self):\n",
    "        with tf.name_scope(\"get_var_fixed_point\"):\n",
    "            # If normalized input length starts at 1.\n",
    "            current_qaa = self.weight_var * tf.constant(\n",
    "                [1.], dtype=tf.float64) + self.bias_var\n",
    "\n",
    "            diff = 1.\n",
    "            prev_qaa_np = 1.\n",
    "            it = 0\n",
    "            while diff > 1e-6 and it < 300:\n",
    "                samp_qaa = interp.interp_lin(\n",
    "                    self.var_aa_grid, self.qaa_grid, current_qaa)\n",
    "                samp_qaa = self.weight_var * samp_qaa + self.bias_var\n",
    "                current_qaa = samp_qaa\n",
    "\n",
    "                with tf.Session() as sess:\n",
    "                    current_qaa_np = sess.run(current_qaa)\n",
    "                diff = np.abs(current_qaa_np - prev_qaa_np)\n",
    "                it += 1\n",
    "                prev_qaa_np = current_qaa_np\n",
    "            return current_qaa_np, current_qaa\n",
    "\n",
    "    def k_diag(self, input_x, return_full=True):\n",
    "        \"\"\"Iteratively building the diagonal part (variance) of the NNGP kernel.\n",
    "\n",
    "    Args:\n",
    "      input_x: tensor of input of size [num_data, input_dim].\n",
    "      return_full: boolean for output to be [num_data] sized or a scalar value\n",
    "        for normalized inputs\n",
    "\n",
    "    Sets self.layer_qaa_dict of {layer #: qaa at the layer}\n",
    "\n",
    "    Returns:\n",
    "      qaa: variance at the output.\n",
    "    \"\"\"\n",
    "        with tf.name_scope(\"Kdiag\"):\n",
    "            # If normalized input length starts at 1.\n",
    "            if self.use_fixed_point_norm:\n",
    "                current_qaa = self.var_fixed_point\n",
    "            else:\n",
    "                current_qaa = self.weight_var * tf.convert_to_tensor(\n",
    "                    [1.], dtype=tf.float64) + self.bias_var\n",
    "            self.layer_qaa_dict = {0: current_qaa}\n",
    "            for l in range(self.depth):\n",
    "                with tf.name_scope(\"layer_%d\" % l):\n",
    "                    samp_qaa = interp.interp_lin(\n",
    "                        self.var_aa_grid, self.qaa_grid, current_qaa)\n",
    "                    samp_qaa = self.weight_var * samp_qaa + self.bias_var\n",
    "                    self.layer_qaa_dict[l + 1] = samp_qaa\n",
    "                    current_qaa = samp_qaa\n",
    "\n",
    "            if return_full:\n",
    "                qaa = tf.tile(current_qaa[:1], ([input_x.shape[0].value]))\n",
    "            else:\n",
    "                qaa = current_qaa[0]\n",
    "            return qaa\n",
    "\n",
    "    def k_full(self, input1, input2=None):\n",
    "        \"\"\"Iteratively building the full NNGP kernel.\n",
    "    \"\"\"\n",
    "        input1 = self._input_layer_normalization(input1)\n",
    "        if input2 is None:\n",
    "            input2 = input1\n",
    "        else:\n",
    "            input2 = self._input_layer_normalization(input2)\n",
    "\n",
    "        with tf.name_scope(\"k_full\"):\n",
    "            cov_init = tf.matmul(\n",
    "                input1, input2, transpose_b=True) / input1.shape[1].value\n",
    "\n",
    "            self.k_diag(input1)\n",
    "            q_aa_init = self.layer_qaa_dict[0]\n",
    "\n",
    "            q_ab = cov_init\n",
    "            q_ab = self.weight_var * q_ab + self.bias_var\n",
    "            corr = q_ab / q_aa_init[0]\n",
    "\n",
    "            if self.fraction_of_int32 > 1:\n",
    "                batch_size, batch_count = self._get_batch_size_and_count(input1, input2)\n",
    "                with tf.name_scope(\"q_ab\"):\n",
    "                    q_ab_all = []\n",
    "                    for b_x in range(batch_count):\n",
    "                        with tf.name_scope(\"batch_%d\" % b_x):\n",
    "                            corr_flat_batch = corr[\n",
    "                                              batch_size * b_x: batch_size * (b_x + 1), :]\n",
    "                            corr_flat_batch = tf.reshape(corr_flat_batch, [-1])\n",
    "\n",
    "                            for l in range(self.depth):\n",
    "                                with tf.name_scope(\"layer_%d\" % l):\n",
    "                                    q_aa = self.layer_qaa_dict[l]\n",
    "                                    q_ab = interp.interp_lin_2d(x=self.var_aa_grid,\n",
    "                                                                y=self.corr_ab_grid,\n",
    "                                                                z=self.qab_grid,\n",
    "                                                                xp=q_aa,\n",
    "                                                                yp=corr_flat_batch)\n",
    "\n",
    "                                    q_ab = self.weight_var * q_ab + self.bias_var\n",
    "                                    corr_flat_batch = q_ab / self.layer_qaa_dict[l + 1][0]\n",
    "\n",
    "                            q_ab_all.append(q_ab)\n",
    "\n",
    "                    q_ab_all = tf.parallel_stack(q_ab_all)\n",
    "            else:\n",
    "                with tf.name_scope(\"q_ab\"):\n",
    "                    corr_flat = tf.reshape(corr, [-1])\n",
    "                    for l in range(self.depth):\n",
    "                        with tf.name_scope(\"layer_%d\" % l):\n",
    "                            q_aa = self.layer_qaa_dict[l]\n",
    "                            q_ab = interp.interp_lin_2d(x=self.var_aa_grid,\n",
    "                                                        y=self.corr_ab_grid,\n",
    "                                                        z=self.qab_grid,\n",
    "                                                        xp=q_aa,\n",
    "                                                        yp=corr_flat)\n",
    "                            q_ab = self.weight_var * q_ab + self.bias_var\n",
    "                            corr_flat = q_ab / self.layer_qaa_dict[l + 1][0]\n",
    "                        q_ab_all = q_ab\n",
    "\n",
    "        return tf.reshape(q_ab_all, cov_init.shape, \"qab\")\n",
    "\n",
    "    def _input_layer_normalization(self, x):\n",
    "        \"\"\"Input normalization to unit variance or fixed point variance.\n",
    "    \"\"\"\n",
    "        with tf.name_scope(\"input_layer_normalization\"):\n",
    "            # Layer norm, fix to unit variance\n",
    "            eps = 1e-15\n",
    "            mean, var = tf.nn.moments(x, axes=[1], keep_dims=True)\n",
    "            x_normalized = (x - mean) / tf.sqrt(var + eps)\n",
    "            if self.use_fixed_point_norm:\n",
    "                x_normalized *= tf.sqrt(\n",
    "                    (self.var_fixed_point[0] - self.bias_var) / self.weight_var)\n",
    "            return x_normalized\n",
    "\n",
    "    def _get_batch_size_and_count(self, input1, input2):\n",
    "        \"\"\"Compute batch size and number to split when input size is large.\n",
    "\n",
    "    Args:\n",
    "      input1: tensor, input tensor to covariance matrix\n",
    "      input2: tensor, second input tensor to covariance matrix\n",
    "\n",
    "    Returns:\n",
    "      batch_size: int, size of each batch\n",
    "      batch_count: int, number of batches\n",
    "    \"\"\"\n",
    "        input1_size = input1.shape[0].value\n",
    "        input2_size = input2.shape[0].value\n",
    "\n",
    "        batch_size = min(np.iinfo(np.int32).max // (self.fraction_of_int32 * input2_size), input1_size)\n",
    "        while input1_size % batch_size != 0:\n",
    "            batch_size -= 1\n",
    "\n",
    "        batch_count = input1_size // batch_size\n",
    "        return batch_size, batch_count\n",
    "\n",
    "\n",
    "def _fill_qab_slice(idx, z1, z2, var_aa, corr_ab, nonlin_fn):\n",
    "    \"\"\"Helper method used for parallel computation for full qab.\"\"\"\n",
    "    log_weights_ab_unnorm = -(z1 ** 2 + z2 ** 2 - 2 * z1 * z2 * corr_ab) / (\n",
    "            2 * var_aa[idx] * (1 - corr_ab ** 2))\n",
    "    log_weights_ab = log_weights_ab_unnorm - tf.reduce_logsumexp(\n",
    "        log_weights_ab_unnorm, axis=[0, 1], keep_dims=True)\n",
    "    weights_ab = tf.exp(log_weights_ab)\n",
    "\n",
    "    qab_slice = tf.reduce_sum(\n",
    "        nonlin_fn(z1) * nonlin_fn(z2) * weights_ab, axis=[0, 1])\n",
    "    qab_slice = tf.Print(qab_slice, [idx], \"Generating slice: \")\n",
    "    return qab_slice\n",
    "\n",
    "\n",
    "def _compute_qmap_grid(nonlin_fn,\n",
    "                       n_gauss,\n",
    "                       n_var,\n",
    "                       n_corr,\n",
    "                       log_spacing=False,\n",
    "                       min_var=1e-8,\n",
    "                       max_var=100.,\n",
    "                       max_corr=0.99999,\n",
    "                       max_gauss=10.):\n",
    "    \"\"\"Construct graph for covariance grid to use for kernel computation.\n",
    "\n",
    "  Given variance and correlation (or covariance) of pre-activation, perform\n",
    "  Gaussian integration to get covariance of post-activation.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if n_gauss is even integer.\n",
    "\n",
    "  Args:\n",
    "    nonlin_fn: tf ops corresponding to point-wise non-linearity in\n",
    "      corresponding NN. e.g.) tf.nn.relu, tf.nn.sigmoid,\n",
    "      lambda x: x * tf.nn.sigmoid(x), ...\n",
    "    n_gauss: int, number of Gaussian integration points with equal spacing\n",
    "      between (-max_gauss, max_gauss). Choose odd integer, so that there is a\n",
    "      gridpoint at 0.\n",
    "    n_var: int, number of variance grid points.get_grid\n",
    "    n_corr: int, number of correlation grid points.\n",
    "    log_spacing: bool, whether to use log-linear instead of linear variance\n",
    "      grid.\n",
    "    min_var: float, smallest variance value to generate grid.\n",
    "    max_var: float, largest varaince value to generate grid.\n",
    "    max_corr: float, largest correlation value to generate grid. Should be\n",
    "      slightly smaller than 1.\n",
    "    max_gauss: float, range (-max_gauss, max_gauss) for Gaussian integration.\n",
    "\n",
    "  Returns:\n",
    "    var_grid_pts: tensor of size [n_var], grid points where variance are\n",
    "      evaluated at.\n",
    "    corr_grid_pts: tensor of size [n_corr], grid points where correlation are\n",
    "      evalutated at.\n",
    "    qaa: tensor of size [n_var], variance of post-activation at given\n",
    "      pre-activation variance.\n",
    "    qab: tensor of size [n_var, n_corr], covariance of post-activation at\n",
    "      given pre-activation variance and correlation.\n",
    "  \"\"\"\n",
    "    if n_gauss % 2 != 1:\n",
    "        raise ValueError(\"n_gauss=%d should be an odd integer\" % n_gauss)\n",
    "\n",
    "    with tf.name_scope(\"compute_qmap_grid\"):\n",
    "        min_var = tf.convert_to_tensor(min_var, dtype=tf.float64)\n",
    "        max_var = tf.convert_to_tensor(max_var, dtype=tf.float64)\n",
    "        max_corr = tf.convert_to_tensor(max_corr, dtype=tf.float64)\n",
    "        max_gauss = tf.convert_to_tensor(max_gauss, dtype=tf.float64)\n",
    "\n",
    "        # Evaluation points for numerical integration over a Gaussian.\n",
    "        z1 = tf.reshape(tf.linspace(-max_gauss, max_gauss, n_gauss), (-1, 1, 1))\n",
    "        z2 = tf.transpose(z1, perm=[1, 0, 2])\n",
    "\n",
    "        if log_spacing:\n",
    "            var_aa = tf.exp(tf.linspace(tf.log(min_var), tf.log(max_var), n_var))\n",
    "        else:\n",
    "            # Evaluation points for pre-activations variance and correlation\n",
    "            var_aa = tf.linspace(min_var, max_var, n_var)\n",
    "        corr_ab = tf.reshape(tf.linspace(-max_corr, max_corr, n_corr), (1, 1, -1))\n",
    "\n",
    "        # compute q_aa\n",
    "        log_weights_aa_unnorm = -0.5 * (z1 ** 2 / tf.reshape(var_aa, [1, 1, -1]))\n",
    "        log_weights_aa = log_weights_aa_unnorm - tf.reduce_logsumexp(\n",
    "            log_weights_aa_unnorm, axis=[0, 1], keep_dims=True)\n",
    "        weights_aa = tf.exp(log_weights_aa)\n",
    "        qaa = tf.reduce_sum(nonlin_fn(z1) ** 2 * weights_aa, axis=[0, 1])\n",
    "\n",
    "        # compute q_ab\n",
    "        # weights to reweight uniform samples by, for q_ab.\n",
    "        # (weights are probability of z1, z2 under Gaussian\n",
    "        #  w/ variance var_aa and covariance var_aa*corr_ab)\n",
    "        # weights_ab will have shape [n_g, n_g, n_v, n_c]\n",
    "        def fill_qab_slice(idx):\n",
    "            return _fill_qab_slice(idx, z1, z2, var_aa, corr_ab, nonlin_fn)\n",
    "\n",
    "        qab = tf.map_fn(\n",
    "            fill_qab_slice,\n",
    "            tf.range(n_var),\n",
    "            dtype=tf.float64,\n",
    "            parallel_iterations=multiprocessing.cpu_count())\n",
    "\n",
    "        var_grid_pts = tf.reshape(var_aa, [-1])\n",
    "        corr_grid_pts = tf.reshape(corr_ab, [-1])\n",
    "\n",
    "        return var_grid_pts, corr_grid_pts, qaa, qab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gaussian process regression model based on GPflow.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class GaussianProcessRegression(object):\n",
    "    \"\"\"Gaussian process regression model based on GPflow.\n",
    "\n",
    "    Args:\n",
    "      input_x: numpy array, [data_size, input_dim]\n",
    "      output_x: numpy array, [data_size, output_dim]\n",
    "      kern: NNGPKernel class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_x, output_y, kern, print_kernel=False):\n",
    "        with tf.name_scope(\"init\"):\n",
    "            self.input_x = input_x\n",
    "            self.output_y = output_y\n",
    "            self.num_train, self.input_dim = input_x.shape\n",
    "            _, self.output_dim = output_y.shape\n",
    "\n",
    "            self.kern = kern\n",
    "            self.print_kernel = print_kernel\n",
    "            self.stability_eps = tf.identity(tf.placeholder(tf.float64))\n",
    "            self.current_stability_eps = 1e-10\n",
    "\n",
    "            self.y_pl = tf.placeholder(tf.float64, [self.num_train, self.output_dim], name=\"y_train\")\n",
    "            self.x_pl = tf.identity(tf.placeholder(tf.float64, [self.num_train, self.input_dim], name=\"x_train\"))\n",
    "\n",
    "            self.l_np = None\n",
    "            self.v_np = None\n",
    "            self.k_np = None\n",
    "\n",
    "        self.k_data_data = tf.identity(self.kern.k_full(self.x_pl))\n",
    "\n",
    "    def _build_predict(self, n_test, full_cov=False):\n",
    "        with tf.name_scope(\"build_predict\"):\n",
    "            self.x_test_pl = tf.identity(\n",
    "                tf.placeholder(tf.float64, [n_test, self.input_dim], name=\"x_test_pl\")\n",
    "            )\n",
    "\n",
    "        tf.logging.info(\"Using pre-computed Kernel\")\n",
    "        self.k_data_test = self.kern.k_full(self.x_pl, self.x_test_pl)\n",
    "\n",
    "        with tf.name_scope(\"build_predict\"):\n",
    "            a = tf.matrix_triangular_solve(self.l, self.k_data_test)\n",
    "            fmean = tf.matmul(a, self.v, transpose_a=True)\n",
    "\n",
    "            if full_cov:\n",
    "                fvar = self.kern.k_full(self.x_test_pl) - tf.matmul(\n",
    "                    a, a, transpose_a=True)\n",
    "                shape = [1, 1, self.y_pl.shape[1]]\n",
    "                fvar = tf.tile(tf.expand_dims(fvar, 2), shape)\n",
    "            else:\n",
    "                fvar = self.kern.k_diag(self.x_test_pl) - tf.reduce_sum(tf.square(a), 0)\n",
    "                fvar = tf.tile(tf.reshape(fvar, (-1, 1)), [1, self.output_y.shape[1]])\n",
    "\n",
    "            self.fmean = fmean\n",
    "            self.fvar = fvar\n",
    "\n",
    "    def _build_cholesky(self):\n",
    "        tf.logging.info(\"Computing Kernel\")\n",
    "        self.k_data_data_reg = self.k_data_data + tf.eye(\n",
    "            self.input_x.shape[0], dtype=tf.float64) * self.stability_eps\n",
    "        if self.print_kernel:\n",
    "            self.k_data_data_reg = tf.Print(\n",
    "                self.k_data_data_reg, [self.k_data_data_reg],\n",
    "                message=\"K_DD = \", summarize=100)\n",
    "        self.l = tf.cholesky(self.k_data_data_reg)\n",
    "        self.v = tf.matrix_triangular_solve(self.l, self.y_pl)\n",
    "\n",
    "    def predict(self, test_x, sess, get_var=False):\n",
    "        \"\"\"Compute mean and varaince prediction for test inputs.\n",
    "\n",
    "        Raises:\n",
    "          ArithmeticError: Cholesky fails even after increasing to large values of\n",
    "            stability epsilon.\n",
    "        \"\"\"\n",
    "        if self.l_np is None:\n",
    "            self._build_cholesky()\n",
    "            start_time = time.time()\n",
    "            self.k_np = sess.run(self.k_data_data,\n",
    "                                 feed_dict={self.x_pl: self.input_x})\n",
    "            tf.logging.info(\"Computed K_DD in %.3f secs\" % (time.time() - start_time))\n",
    "\n",
    "            while self.current_stability_eps < 1:\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    self.l_np, self.v_np = sess.run(\n",
    "                        [self.l, self.v],\n",
    "                        feed_dict={self.y_pl: self.output_y,\n",
    "                                   self.k_data_data: self.k_np,\n",
    "                                   self.stability_eps: self.current_stability_eps})\n",
    "                    tf.logging.info(\n",
    "                        \"Computed L_DD in %.3f secs\" % (time.time() - start_time))\n",
    "                    break\n",
    "\n",
    "                except tf.errors.InvalidArgumentError:\n",
    "                    self.current_stability_eps *= 10\n",
    "                    tf.logging.info(\"Cholesky decomposition failed, trying larger epsilon\"\n",
    "                                    \": {}\".format(self.current_stability_eps))\n",
    "\n",
    "        if self.current_stability_eps > 0.2:\n",
    "            raise ArithmeticError(\"Could not compute Cholesky decomposition.\")\n",
    "\n",
    "        n_test = test_x.shape[0]\n",
    "        self._build_predict(n_test)\n",
    "        feed_dict = {\n",
    "            self.x_pl: self.input_x,\n",
    "            self.x_test_pl: test_x,\n",
    "            self.l: self.l_np,\n",
    "            self.v: self.v_np\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        if get_var:\n",
    "            mean_pred, var_pred = sess.run(\n",
    "                [self.fmean, self.fvar], feed_dict=feed_dict)\n",
    "            tf.logging.info(\"Did regression in %.3f secs\" % (time.time() - start_time))\n",
    "            return mean_pred, var_pred, self.current_stability_eps\n",
    "\n",
    "        else:\n",
    "            mean_pred = sess.run(self.fmean, feed_dict=feed_dict)\n",
    "            tf.logging.info(\"Did regression in %.3f secs\" % (time.time() - start_time))\n",
    "            return mean_pred, self.current_stability_eps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mldata.org dataset: mnist-original'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='mnist')\n",
    "mnist.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(mnist.target)\n",
    "# encode target labels as zero-mean one hot encoded vector \n",
    "# with negative class = -0.1 and positive class as 0.9\n",
    "encode = lambda y: lb.transform(y) - .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train, image_test, label_train, label_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify = mnist.target, random_state = 444, test_size=.15)\n",
    "\n",
    "X_test = image_test\n",
    "y_test = encode(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_subset(n):\n",
    "    x, _, y, _ = train_test_split(image_train, label_train, stratify = label_train, random_state=333, train_size = n)\n",
    "    return x, encode(y)\n",
    "\n",
    "def get_test_subset(n):\n",
    "    _, x, _, y = train_test_split(image_train, label_train, stratify = label_train, random_state=333, train_size = n)\n",
    "    return x, encode(y)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    return np.mean(np.argmax(y, axis = 1) == np.argmax(y_hat, axis = 1))\n",
    "\n",
    "def do_nngp_experiment(layers, sample_size):\n",
    "    X_train, y_train = get_train_subset(sample_size)\n",
    "    config = tf.ConfigProto(\n",
    "           intra_op_parallelism_threads = 32,\n",
    "           inter_op_parallelism_threads = 32,\n",
    "           allow_soft_placement = True,\n",
    "           device_count = {'CPU' : 32, 'GPU' : 0}\n",
    "        )\n",
    "    with tf.Session(config=config) as sess:\n",
    "        nngp_kernel = NNGPKernel(\n",
    "            depth=layers,\n",
    "            weight_var=1.79,\n",
    "            bias_var=0.83,\n",
    "            nonlin_fn=tf.tanh,\n",
    "            grid_path='grid',\n",
    "            use_precomputed_grid=True,\n",
    "            n_gauss=501,\n",
    "            n_var=501,\n",
    "            n_corr=501,\n",
    "            max_gauss=10,\n",
    "            max_var=100,\n",
    "            use_fixed_point_norm=False)\n",
    "\n",
    "        model = GaussianProcessRegression(X_train, y_train, kern=nngp_kernel)\n",
    "        y_hat, test_eps = model.predict(X_test, sess)\n",
    "\n",
    "    return mean_squared_error(y_test, y_hat), accuracy(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def train_or_load_nn_model(nn_width, nn_depth, sample_size):\n",
    "    model_path = 'models/mnist_w%d_s%d' % (nn_width, sample_size)\n",
    "    config = tf.ConfigProto()\n",
    "    set_session(tf.Session(config=config))\n",
    "\n",
    "    if (os.path.isfile(model_path)):\n",
    "        model = load_model(model_path)\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(nn_width, input_dim=784, kernel_initializer='normal', activation='tanh'))\n",
    "        for i in np.arange(1, nn_depth):\n",
    "            model.add(Dense(nn_width, kernel_initializer='normal', activation='tanh'))\n",
    "        model.add(Dense(10, kernel_initializer='normal'))\n",
    "        model.compile(loss='mean_squared_error', optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True))\n",
    "        X_train, y_train = get_train_subset(sample_size)\n",
    "        model.fit(X_train, y_train, epochs = 100, batch_size = 256, verbose = 0)\n",
    "        #model.save(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.array([1000, 3000, 5000,  10000])\n",
    "#sample_sizes = np.array([1000, 3000, 5000,  10000, 20000, 50000])\n",
    "nn_depths    = np.array([8,    8,    4,     4,     2,     1])\n",
    "nn_widths = np.array([8, 16, 64, 128, 512, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 1000, Depth: 8, Width: 8\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 1000, Depth: 8, Width: 16\n",
      "Sample Size: 1000, Depth: 8, Width: 64\n",
      "Sample Size: 1000, Depth: 8, Width: 128\n",
      "Sample Size: 1000, Depth: 8, Width: 512\n",
      "Sample Size: 1000, Depth: 8, Width: 1024\n",
      "done sample size 1000\n",
      "Sample Size: 3000, Depth: 8, Width: 8\n",
      "Sample Size: 3000, Depth: 8, Width: 16\n",
      "Sample Size: 3000, Depth: 8, Width: 64\n",
      "Sample Size: 3000, Depth: 8, Width: 128\n",
      "Sample Size: 3000, Depth: 8, Width: 512\n",
      "Sample Size: 3000, Depth: 8, Width: 1024\n",
      "done sample size 3000\n",
      "Sample Size: 5000, Depth: 4, Width: 8\n",
      "Sample Size: 5000, Depth: 4, Width: 16\n",
      "Sample Size: 5000, Depth: 4, Width: 64\n",
      "Sample Size: 5000, Depth: 4, Width: 128\n",
      "Sample Size: 5000, Depth: 4, Width: 512\n",
      "Sample Size: 5000, Depth: 4, Width: 1024\n",
      "done sample size 5000\n",
      "Sample Size: 10000, Depth: 4, Width: 8\n",
      "Sample Size: 10000, Depth: 4, Width: 16\n",
      "Sample Size: 10000, Depth: 4, Width: 64\n",
      "Sample Size: 10000, Depth: 4, Width: 128\n",
      "Sample Size: 10000, Depth: 4, Width: 512\n",
      "Sample Size: 10000, Depth: 4, Width: 1024\n",
      "done sample size 10000\n"
     ]
    }
   ],
   "source": [
    "#nn_results = np.empty((sample_sizes.shape[0], nn_widths.shape[0], 2))\n",
    "columns = ['Model Name', 'Sample Size', 'MSE', 'Accuracy']\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    for j, nn_width in enumerate(nn_widths):\n",
    "        print('Sample Size: {}, Depth: {}, Width: {}'.format(sample_size, nn_depths[i], nn_width))\n",
    "        m = train_or_load_nn_model(nn_width = nn_width, nn_depth = nn_depths[i], sample_size = sample_size)\n",
    "        y_hat = m.predict(X_test)\n",
    "        model_name = 'NN-{}'.format(nn_width)\n",
    "        mse_result = accuracy(y_test, y_hat)  \n",
    "        acc_result = mean_squared_error(y_test, y_hat)\n",
    "        #nn_results[i, j, 0] = accuracy(y_test, y_hat)  \n",
    "        #nn_results[i, j, 1] = mean_squared_error(y_test, y_hat)\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([[model_name, sample_size, mse_result, acc_result]], columns=columns)], ignore_index=True)\n",
    "    print('done sample size %d' % sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1000, Depth: 8\n",
      "INFO:tensorflow:Loaded interpolation grid from grid/grid_tanh_ng501_ns502_nc501_mv100_mg10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From nngp/interp.py:91: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Computing Kernel\n",
      "INFO:tensorflow:Computed K_DD in 2.289 secs\n",
      "INFO:tensorflow:Computed L_DD in 0.148 secs\n",
      "INFO:tensorflow:Using pre-computed Kernel\n",
      "INFO:tensorflow:Did regression in 9.106 secs\n",
      "Sample size: 3000, Depth: 8\n",
      "INFO:tensorflow:Loaded interpolation grid from grid/grid_tanh_ng501_ns502_nc501_mv100_mg10\n",
      "INFO:tensorflow:Computing Kernel\n",
      "INFO:tensorflow:Computed K_DD in 7.506 secs\n",
      "INFO:tensorflow:Computed L_DD in 0.596 secs\n",
      "INFO:tensorflow:Using pre-computed Kernel\n",
      "INFO:tensorflow:Did regression in 27.044 secs\n",
      "Sample size: 5000, Depth: 4\n",
      "INFO:tensorflow:Loaded interpolation grid from grid/grid_tanh_ng501_ns502_nc501_mv100_mg10\n",
      "INFO:tensorflow:Computing Kernel\n",
      "INFO:tensorflow:Computed K_DD in 9.930 secs\n",
      "INFO:tensorflow:Computed L_DD in 2.043 secs\n",
      "INFO:tensorflow:Using pre-computed Kernel\n",
      "INFO:tensorflow:Did regression in 26.900 secs\n",
      "Sample size: 10000, Depth: 4\n",
      "INFO:tensorflow:Loaded interpolation grid from grid/grid_tanh_ng501_ns502_nc501_mv100_mg10\n",
      "INFO:tensorflow:Computing Kernel\n",
      "INFO:tensorflow:Computed K_DD in 34.901 secs\n",
      "INFO:tensorflow:Computed L_DD in 12.986 secs\n",
      "INFO:tensorflow:Using pre-computed Kernel\n",
      "INFO:tensorflow:Did regression in 67.841 secs\n"
     ]
    }
   ],
   "source": [
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    print('Sample size: {}, Depth: {}'.format(sample_size, nn_depths[i]))\n",
    "    mse_result, acc_result = do_nngp_experiment(layers=nn_depths[i], sample_size=sample_size)\n",
    "    model_name = 'NNGP'\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([[model_name, sample_size, mse_result, acc_result]], columns=columns)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Sample Size</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN-8</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN-16</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN-64</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN-128</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.449429</td>\n",
       "      <td>0.086094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN-512</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.732762</td>\n",
       "      <td>0.065514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NN-1024</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.624667</td>\n",
       "      <td>0.118871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN-8</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NN-16</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN-64</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NN-128</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.061470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN-512</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.858762</td>\n",
       "      <td>0.038980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NN-1024</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.816952</td>\n",
       "      <td>0.060635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN-8</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NN-16</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.259619</td>\n",
       "      <td>0.084578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NN-64</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.804476</td>\n",
       "      <td>0.043265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NN-128</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.833619</td>\n",
       "      <td>0.039694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN-512</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.874095</td>\n",
       "      <td>0.038664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NN-1024</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.866762</td>\n",
       "      <td>0.047225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NN-8</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.112571</td>\n",
       "      <td>0.089905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NN-16</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.355238</td>\n",
       "      <td>0.073035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NN-64</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.884190</td>\n",
       "      <td>0.025248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NN-128</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.892571</td>\n",
       "      <td>0.024218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NN-512</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.912476</td>\n",
       "      <td>0.026433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NN-1024</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.906857</td>\n",
       "      <td>0.031869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NNGP</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.020460</td>\n",
       "      <td>0.928762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NNGP</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>0.954571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NNGP</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.962952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NNGP</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0.973048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name Sample Size       MSE  Accuracy\n",
       "0        NN-8        1000  0.112571  0.089971\n",
       "1       NN-16        1000  0.112571  0.089971\n",
       "2       NN-64        1000  0.112571  0.089938\n",
       "3      NN-128        1000  0.449429  0.086094\n",
       "4      NN-512        1000  0.732762  0.065514\n",
       "5     NN-1024        1000  0.624667  0.118871\n",
       "6        NN-8        3000  0.112571  0.089971\n",
       "7       NN-16        3000  0.112571  0.089971\n",
       "8       NN-64        3000  0.112571  0.089938\n",
       "9      NN-128        3000  0.624000  0.061470\n",
       "10     NN-512        3000  0.858762  0.038980\n",
       "11    NN-1024        3000  0.816952  0.060635\n",
       "12       NN-8        5000  0.112571  0.089955\n",
       "13      NN-16        5000  0.259619  0.084578\n",
       "14      NN-64        5000  0.804476  0.043265\n",
       "15     NN-128        5000  0.833619  0.039694\n",
       "16     NN-512        5000  0.874095  0.038664\n",
       "17    NN-1024        5000  0.866762  0.047225\n",
       "18       NN-8       10000  0.112571  0.089905\n",
       "19      NN-16       10000  0.355238  0.073035\n",
       "20      NN-64       10000  0.884190  0.025248\n",
       "21     NN-128       10000  0.892571  0.024218\n",
       "22     NN-512       10000  0.912476  0.026433\n",
       "23    NN-1024       10000  0.906857  0.031869\n",
       "24       NNGP        1000  0.020460  0.928762\n",
       "25       NNGP        3000  0.013834  0.954571\n",
       "26       NNGP        5000  0.011747  0.962952\n",
       "27       NNGP       10000  0.009235  0.973048"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (14, 5))\n",
    "ax1 = plt.subplot(121)\n",
    "results_df.pivot(index='Sample Size', columns='Model Name', values='Accuracy').plot(ax=ax1)\n",
    "plt.ylabel('accuracy')\n",
    "ax2 = plt.subplot(122)\n",
    "results_df.pivot(index='Sample Size', columns='Model Name', values='MSE').plot(ax=ax2)\n",
    "plt.ylabel('mse')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
