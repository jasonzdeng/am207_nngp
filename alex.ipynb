{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review and tutorial of \n",
    "\n",
    "# Deep Neural Networks as Gaussian Processes \n",
    "Jaehoon Lee, Yasaman Bahri, Roman Novak , Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein\n",
    "  \n",
    "#### Presented by  \n",
    "[Jason Deng](mailto:dengzj@Hotmail.com)  \n",
    "[Alexander Dubitskiy](mailto:ald028@g.harvard.edu)  \n",
    "[Zheng Yang](mailto:zhengyang@g.harvard.edu)  \n",
    "[Sean Tierney](mailto:set936@g.harvard.edu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question of defining meaningful priors for a neural network was first addressed by Radford M. Neal in his 1994 paper [Priors for infinite networks](ftp://www.cs.toronto.edu/dist/radford/pin.pdf).  \n",
    "In the paper he studied a neural network with a real valued input of size I, one hidden layer of size H with sigmoidal transfer function and real valued output of size O. This network can be described with the following equations:\n",
    "$$ f_k(x) = b_k + \\sum_{i=1}^{H} v_{jk} h_j(x) $$ \n",
    "$$ h_j(x) = tanh( a_j + \\sum_{i=1}^{I} u_{ij} x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author suggested Gaussian priors over the network weights $ b_k \\propto \\mathcal{N}(0, \\sigma_b) , v_{jk} \\propto \\mathcal{N}(0, \\sigma_v), a_j \\propto \\mathcal{N}(0, \\sigma_a) , u_{ij} \\propto \\mathcal{N}(0, \\sigma_u) $ and showed that when $H \\to \\infty$ the prior joint distribution converges to a multivariate Gaussian with zero means and covarinace:\n",
    "  \n",
    "$$ E[f_k(x) f_k(x\\prime)]  = \\sigma_{b}^2 + \\sum_{j} \\sigma_{v}^2 E[h_{j}(x)h_{j}(x \\prime)] = \\sigma_{b}^2 + w_{v}^2 C(x, x \\prime) $$\n",
    "and this is a Gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work was further extended by Christopher K. I. Williams in his 1996 paper [Computing with infinite networks](https://papers.nips.cc/paper/1197-computing-with-infinite-networks.pdf).  \n",
    "Williams showed how to calculate the corresponding kernels for single-layered neural networks with sigmoidal and Gaussian transfer functions.  \n",
    "  \n",
    "$$ C_{erf}(x, x \\prime) = \\frac{2}{\\pi} \\sin^{-1} \\frac{2 x^{T} \\Sigma x \\prime }{\\sqrt{(1 + 2 x^{T} \\Sigma x ) (1 + 2 x \\prime^{T} \\Sigma x \\prime )}} $$\n",
    "  \n",
    "$$ C_{G}(x, x \\prime) = (\\frac{\\sigma_e}{\\sigma_u})^d \\exp(- \\frac{x^{T}x}{2 \\sigma_{m}^2}) \\exp(- \\frac{(x - x^{T})^{T} (x - x^{T})}{2 \\sigma_{s}^2}) \\exp(- \\frac{x \\prime^{T}x \\prime}{2 \\sigma_{m}^2}) $$  \n",
    "where \n",
    "$$ \\frac{1}{\\sigma_{e}^2} = \\frac{2}{\\sigma_{g}^2} + \\frac{1}{\\sigma_{u}^2} , \\sigma_{s}^2 = 2 \\sigma_{g}^2 + \\frac{\\sigma_{g}^4}{\\sigma_{u}^2} , \\sigma_{m}^2 = 2 \\sigma_{u}^2 + \\sigma_{g}^2 $$  \n",
    "and $ \\sigma_{g} $ is the width of the Gaussian transfer function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Summary of the kernel computation for a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
